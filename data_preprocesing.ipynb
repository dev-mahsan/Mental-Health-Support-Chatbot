{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51vmaGl8SqZ3",
        "outputId": "60490da3-74e3-4a64-c308-e83d83bd42bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/nltk_data/tokenizers/punkt\n",
            "Class Distribution Before Handling Minority Classes:\n",
            "emotion_list\n",
            "neutral           11020\n",
            "approval           3523\n",
            "admiration         3413\n",
            "annoyance          2707\n",
            "gratitude          2356\n",
            "disapproval        2329\n",
            "curiosity          1993\n",
            "amusement          1824\n",
            "realization        1760\n",
            "optimism           1749\n",
            "disappointment     1672\n",
            "anger              1651\n",
            "joy                1588\n",
            "love               1582\n",
            "confusion          1485\n",
            "sadness            1358\n",
            "caring             1232\n",
            "excitement         1160\n",
            "surprise           1110\n",
            "disgust            1033\n",
            "desire              754\n",
            "                    700\n",
            "fear                608\n",
            "remorse             527\n",
            "embarrassment       474\n",
            "nervousness         350\n",
            "relief              284\n",
            "pride               280\n",
            "grief               116\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Classes with less than 6 samples: []\n",
            "\n",
            "Class Distribution After Removing Minority Classes:\n",
            "emotion_list\n",
            "neutral           11020\n",
            "approval           3523\n",
            "admiration         3413\n",
            "annoyance          2707\n",
            "gratitude          2356\n",
            "disapproval        2329\n",
            "curiosity          1993\n",
            "amusement          1824\n",
            "realization        1760\n",
            "optimism           1749\n",
            "disappointment     1672\n",
            "anger              1651\n",
            "joy                1588\n",
            "love               1582\n",
            "confusion          1485\n",
            "sadness            1358\n",
            "caring             1232\n",
            "excitement         1160\n",
            "surprise           1110\n",
            "disgust            1033\n",
            "desire              754\n",
            "                    700\n",
            "fear                608\n",
            "remorse             527\n",
            "embarrassment       474\n",
            "nervousness         350\n",
            "relief              284\n",
            "pride               280\n",
            "grief               116\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Data preprocessing complete. Preprocessed data saved to 'preprocessed_data.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.data.path.append('/usr/nltk_data')\n",
        "\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "try:\n",
        "    print(nltk.data.find('tokenizers/punkt'))\n",
        "except Exception as e:\n",
        "    print(\"Error downloading punkt:\", e)\n",
        "    print(\"Trying alternate locations or manual download.\")\n",
        "\n",
        "go_emotions_1 = pd.read_csv('goemotions_1.csv')\n",
        "go_emotions_2 = pd.read_csv('goemotions_2.csv')\n",
        "go_emotions_3 = pd.read_csv('goemotions_3.csv')\n",
        "\n",
        "go_emotions_1_sampled = go_emotions_1.sample(frac=0.2, random_state=42)\n",
        "go_emotions_2_sampled = go_emotions_2.sample(frac=0.2, random_state=42)\n",
        "go_emotions_3_sampled = go_emotions_3.sample(frac=0.2, random_state=42)\n",
        "\n",
        "all_go_emotions = pd.concat([go_emotions_1_sampled, go_emotions_2_sampled, go_emotions_3_sampled], ignore_index=True)\n",
        "\n",
        "emotion_columns = [\n",
        "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
        "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
        "    'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love',\n",
        "    'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
        "    'sadness', 'surprise', 'neutral'\n",
        "]\n",
        "\n",
        "def get_emotion_labels(row):\n",
        "    emotions = []\n",
        "    for emotion in emotion_columns:\n",
        "        if row[emotion] == 1:\n",
        "            emotions.append(emotion)\n",
        "    return ','.join(emotions)\n",
        "\n",
        "all_go_emotions['emotion'] = all_go_emotions.apply(get_emotion_labels, axis=1)\n",
        "\n",
        "all_go_emotions = all_go_emotions[['text', 'emotion']]\n",
        "\n",
        "combined_df = all_go_emotions.copy()\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
        "    return text\n",
        "\n",
        "combined_df['text'] = combined_df['text'].apply(normalize_text)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "combined_df['tokens'] = combined_df['text'].apply(tokenizer.tokenize)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "combined_df['tokens'] = combined_df['tokens'].apply(remove_stopwords)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "combined_df['tokens'] = combined_df['tokens'].apply(lemmatize_tokens)\n",
        "\n",
        "combined_df['emotion_list'] = combined_df['emotion'].apply(lambda x: x.split(','))\n",
        "\n",
        "exploded_df = combined_df.explode('emotion_list')\n",
        "\n",
        "exploded_df['emotion_list'] = exploded_df['emotion_list'].str.strip()\n",
        "\n",
        "emotion_counts = exploded_df['emotion_list'].value_counts()\n",
        "print(\"Class Distribution Before Handling Minority Classes:\")\n",
        "print(emotion_counts)\n",
        "\n",
        "class_counts = exploded_df['emotion_list'].value_counts()\n",
        "minority_classes = class_counts[class_counts < 6].index.tolist()\n",
        "print(f\"\\nClasses with less than 6 samples: {minority_classes}\")\n",
        "\n",
        "filtered_df = exploded_df[~exploded_df['emotion_list'].isin(minority_classes)].reset_index(drop=True)\n",
        "\n",
        "print(\"\\nClass Distribution After Removing Minority Classes:\")\n",
        "print(filtered_df['emotion_list'].value_counts())\n",
        "\n",
        "filtered_df['processed_text'] = filtered_df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(filtered_df['processed_text'])\n",
        "y = filtered_df['emotion_list']\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "resampled_df = pd.DataFrame({\n",
        "    'processed_text': [' '.join(tokens) for tokens in vectorizer.inverse_transform(X_resampled)],\n",
        "    'emotion': y_resampled\n",
        "})\n",
        "\n",
        "resampled_df.to_csv('preprocessed_data.csv', index=False)\n",
        "\n",
        "print(\"\\nData preprocessing complete. Preprocessed data saved to 'preprocessed_data.csv'.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
